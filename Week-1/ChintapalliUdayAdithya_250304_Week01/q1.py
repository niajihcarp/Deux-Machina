# -*- coding: utf-8 -*-
"""Q1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qJFd3DAQCRCJ0XiRELzIsjZX266akWJa
"""

import gymnasium as gym
import numpy as np
import random
custom_map = [
    "SFFF",
    "FHFH",
    "FFFH",
    "HFFG"
]
env = gym.make("FrozenLake-v1",desc=custom_map, is_slippery=True)

state_space_size = env.observation_space.n
action_space_size = env.action_space.n

print("State space:", state_space_size)
print("Action space:", action_space_size)
q_table = np.zeros((state_space_size, action_space_size))
episodes = 30000
max_steps_per_episode = 2000

learning_rate = 0.8
discount_factor = 0.95

epsilon_start = 1.0
epsilon_end = 0.001
epsilon_decay = 0.999
rewards_all_episodes = []

for episode in range(episodes):
    state, _ = env.reset()
    total_rewards = 0

    epsilon = max(
        epsilon_end,
        epsilon_start * (epsilon_decay ** episode)
    )

    for step in range(max_steps_per_episode):


        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()
        else:
            action = np.argmax(q_table[state, :])

        new_state, reward, terminated, truncated, _ = env.step(action)


        q_table[state, action] = q_table[state, action] + learning_rate * (
            reward
            + discount_factor * np.max(q_table[new_state, :])
            - q_table[state, action]
        )

        state = new_state
        total_rewards += reward


        if terminated or truncated:
            break

    rewards_all_episodes.append(total_rewards)

"""**Training**"""

print("Average reward per 1000 episodes:")
for i in range(0, episodes, 1000):
    avg_reward = np.mean(rewards_all_episodes[i:i+1000])
    print(f"{i} - {i+1000}: {avg_reward}")

"""# Testing"""

test_episodes = 100
success_count = 0

for episode in range(test_episodes):
    state, _ = env.reset()

    for step in range(max_steps_per_episode):


        action = np.argmax(q_table[state, :])

        new_state, reward, terminated, truncated, _ = env.step(action)

        state = new_state


        if terminated or truncated:
            if reward == 1:
                success_count += 1
            break

print(f"Success rate over {test_episodes} episodes: {success_count}%")

print("Optimal policy (action per state):")
print(np.argmax(q_table, axis=1))