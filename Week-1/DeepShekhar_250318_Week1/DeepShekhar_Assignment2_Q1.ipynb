{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce031d0-c4d6-451b-85b1-b13413f7c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05bf3344-aa63-4500-8e57-427e7249dd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 16\n",
      "Actions: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=None)\n",
    "state, info = env.reset()\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(\"States:\", state_size)\n",
    "print(\"Actions:\", action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca52c0-77d0-46f4-92f7-99a90de6e3f6",
   "metadata": {},
   "source": [
    "## Parameters:\n",
    "\n",
    "- Learning rate (alpha) = 0.80 \n",
    "- Discount factor (gamma) = 0.95  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed86292-88f8-487f-a61d-42fb0346371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.80\n",
    "gamma = 0.95\n",
    "\n",
    "eps_start = 1.0    # High initial eps value -> for full exploration initially\n",
    "eps_end = 0.05    # Low final eps value -> for almost full exploitation\n",
    "eps_decay = 0.9995    # Controlling at what pace does exploration reduces\n",
    "\n",
    "episodes = 5000\n",
    "max_steps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9700b232-4f21-4be0-a247-042459e223c0",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Strategy:\n",
    "\n",
    "Epsilon is decayed exponentially using the formula:\n",
    "\n",
    "{ epsilon = max(eps_end, eps_start Ã— ((eps_decay)^episode)) }\n",
    "\n",
    "In the epsilon-greedy strategy, the agent explores the environment by taking random actions with probability eps. As training progresses, eps decays     exponentially, allowing the agent to exploit the learned Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed5bf6c1-95f6-4ca3-9593-44f0f5e35fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectAction(Q, state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return env.action_space.sample()    # Choosing random action (as exploration)\n",
    "    else:\n",
    "        return np.argmax(Q[state, :])    # Choosing action with highest Q-value (as exploitation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5bd82f-6ff4-4510-bae2-f7db95ba65bb",
   "metadata": {},
   "source": [
    "## Training Using Q-Learning:\n",
    "\n",
    "The Q-value update rule is:\n",
    "\n",
    "{ Q[state, action] = Q[state, action] + alpha*(reward + gamma*max(Q[next_state]) - Q[state, action]) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72db256b-af98-42b0-b68f-54989896bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsGreedyQLearning(alpha, gamma):\n",
    "    q_table = np.zeros((state_size, action_size))\n",
    "    \n",
    "    epsilon = eps_start\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, info = env.reset()\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = selectAction(q_table, state, epsilon)\n",
    "\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            max_future_q = np.max(q_table[new_state, :])\n",
    "            old_q_value = q_table[state, action]\n",
    "\n",
    "            q_table[state, action] = old_q_value + alpha * (\n",
    "                reward + gamma * max_future_q - old_q_value\n",
    "            )\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "        epsilon = max(eps_end, eps_start * (eps_decay ** episode))\n",
    "\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea40ca87-5e9d-4c2b-8167-a19c5210c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = epsGreedyQLearning(alpha, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8997dab8-75d5-44b5-b5df-7451dfe14627",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"frozenlake_Assignment2_Q1_qtable.npy\", q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87d25ec5-33b2-45bc-91d1-a523ff174bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.load(\"frozenlake_Assignment2_Q1_qtable.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54691831-eb88-4c21-8078-b1df863f9695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Test successful!! <--\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\")\n",
    "state, info = env.reset()\n",
    "\n",
    "done = False\n",
    "success = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state, :])\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if reward == 1:\n",
    "        success = True\n",
    "        \n",
    "    done = terminated or truncated\n",
    "\n",
    "if success:\n",
    "    print(\"--> Test successful!! <--\")\n",
    "else:\n",
    "    print(\"Test failed!!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
