{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IyGeNUiwEMZt","executionInfo":{"status":"ok","timestamp":1765976397811,"user_tz":-330,"elapsed":1431,"user":{"displayName":"Yuvraj Gupta","userId":"10929797348313081580"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"840b4382","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765976404015,"user_tz":-330,"elapsed":5558,"user":{"displayName":"Yuvraj Gupta","userId":"10929797348313081580"}},"outputId":"f6c37f1a-53e2-430a-dad3-9ea15ea5d7e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium[toy-text] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[toy-text]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[toy-text]) (3.1.2)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[toy-text]) (4.15.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[toy-text]) (0.0.4)\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[toy-text]) (2.6.1)\n"]}],"source":["pip install gymnasium[toy-text]"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"412f2b87","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765976404016,"user_tz":-330,"elapsed":62,"user":{"displayName":"Yuvraj Gupta","userId":"10929797348313081580"}},"outputId":"a636ae88-b236-40c6-9548-03b4e6331ce2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Discrete(16)\n","Discrete(4)\n"]}],"source":["import gymnasium as gym\n","env = gym.make(\"FrozenLake-v1\",\n","    desc=None,\n","    map_name=\"4x4\",\n","    is_slippery=True,\n","    success_rate=1.0/3.0,\n","    reward_schedule=(1, 0, 0),\n","    render_mode='rgb_array') # Changed render_mode to 'rgb_array'\n","print(env.observation_space)\n","print(env.action_space)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ARiU_4MjMPoV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765976404016,"user_tz":-330,"elapsed":59,"user":{"displayName":"Yuvraj Gupta","userId":"10929797348313081580"}},"outputId":"204f337d-fa66-44bb-904f-44bd6d58d20d"},"outputs":[{"output_type":"stream","name":"stdout","text":["16\n","4\n","0\n","{'prob': 1}\n"]}],"source":["# to get number of observation and action\n","print(env.observation_space.n)\n","print(env.action_space.n)\n","observation , info=env.reset()\n","print(observation) # this is the current state\n","print(info)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"uaLoAPsoMmwO","executionInfo":{"status":"ok","timestamp":1765976404016,"user_tz":-330,"elapsed":44,"user":{"displayName":"Yuvraj Gupta","userId":"10929797348313081580"}}},"outputs":[],"source":["import numpy as np\n","# this is our q learning table with rows =16 and column =4\n","# rows represent our state and column represent our action\n","q_table=np.zeros((env.observation_space.n,env.action_space.n))\n","\n","learning_rate=0.1\n","discount_factor=0.99\n","epsilon=1\n","min_epsilon=0.01\n","max_epsilon=1.0\n","decay_rate=0.001\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"IEno2MibOoXF","executionInfo":{"status":"ok","timestamp":1765976404016,"user_tz":-330,"elapsed":43,"user":{"displayName":"Yuvraj Gupta","userId":"10929797348313081580"}}},"outputs":[],"source":["# now we will implement our epsilon greedy policy for exploration and exploitation\n","\n","#0: Move left\n","#1: Move down\n","#2: Move right\n","#3: Move up\n","\n","def epsilon_greedy_policy(q_table,state,epsilon):\n","    if np.random.uniform(0,1) < epsilon:\n","        action= env.action_space.sample()   # here we explore every possibility-----------exploration\n","\n","    else:\n","        action=np.argmax(q_table[state,:])  # here we Exploit ------------exploitation\n","\n","    return action"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAMhkgGYQfzR","executionInfo":{"status":"ok","timestamp":1765976432532,"user_tz":-330,"elapsed":28558,"user":{"displayName":"Yuvraj Gupta","userId":"10929797348313081580"}},"outputId":"f2835071-109f-44cb-9bf1-19e891f008d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["episode = 1000  epsilon= 0.3745650295675263  reward = 0.04\n","episode = 2000  epsilon= 0.14411597934795192  reward = 0.209\n","episode = 3000  epsilon= 0.059338511534685244  reward = 0.438\n","episode = 4000  epsilon= 0.02815062405161077  reward = 0.559\n","episode = 5000  epsilon= 0.01667724143301951  reward = 0.617\n","episode = 6000  epsilon= 0.012456419846946017  reward = 0.661\n","episode = 7000  epsilon= 0.010903666360576941  reward = 0.685\n","episode = 8000  epsilon= 0.010332440275734477  reward = 0.702\n","episode = 9000  epsilon= 0.010122297942860079  reward = 0.656\n","episode = 10000  epsilon= 0.010044990898875783  reward = 0.659\n","episode = 11000  epsilon= 0.010016551226736224  reward = 0.674\n","episode = 12000  epsilon= 0.010006088856042424  reward = 0.669\n","episode = 13000  epsilon= 0.01000223996495826  reward = 0.704\n","episode = 14000  epsilon= 0.010000824037057088  reward = 0.671\n","episode = 15000  epsilon= 0.010000303146292066  reward = 0.693\n","episode = 16000  epsilon= 0.010000111521288518  reward = 0.683\n","episode = 17000  epsilon= 0.0100000410263893  reward = 0.695\n","episode = 18000  epsilon= 0.010000015092765168  reward = 0.68\n","episode = 19000  epsilon= 0.010000005552318015  reward = 0.681\n","episode = 20000  epsilon= 0.01000000204258365  reward = 0.665\n","\n"," TRANING COMPLETE\n"]}],"source":["# nows comes the main part\n","# in these we will run several episodesusing for loop\n","\n","episodes=20000\n","step_per_episodes=100\n","\n","reward_array=[]\n","\n","for i in range(episodes):\n","    observation , info=env.reset()\n","    truncated=False\n","    terminated=False\n","    episode_reward=0\n","\n","\n","    for j in range(step_per_episodes):\n","        action=epsilon_greedy_policy(q_table,observation,epsilon)\n","        new_observation,reward,terminated,truncated,info=env.step(action)\n","\n","        # now we will implement our q learning using bellman's equation and temporal difference algorithm\n","        q_table[observation, action] = q_table[observation , action] + learning_rate * ( reward + discount_factor * np.max(q_table[new_observation, :]) - q_table[observation, action])\n","\n","        # we just add the reward for every step\n","        episode_reward+=reward\n","        observation=new_observation\n","\n","        # env.render() # Removed env.render() from the training loop\n","\n","        if terminated or truncated:\n","           break\n","\n","\n","    reward_array.append(episode_reward)\n","\n","     # here now we will use epsilon decay method to set our epsilon\n","    epsilon=min_epsilon+(max_epsilon-min_epsilon)*np.exp(-decay_rate*i)\n","\n","    if (i+1)%1000==0:\n","       print(\"episode =\" , i+1 ,\" epsilon=\" , epsilon ,\" reward =\", np.mean(reward_array[-1000:]))\n","\n","\n","print('\\n TRANING COMPLETE')\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1us9sQFEFfuFbEBKWFv2T_wGAD-GXWDZ-","timestamp":1765855455231}],"authorship_tag":"ABX9TyPGCFkZv31ouxK8UPdQbKTP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}